{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6ee1bf1",
   "metadata": {},
   "source": [
    "# Uneven pp\n",
    "\n",
    "## Parameters involved in the configuration file:\n",
    "\n",
    "```\n",
    "        --tensor-model-parallel-size ${TP} \\\n",
    "        --pipeline-model-parallel-size ${PP} \\\n",
    "        --decoder-first-pipeline-num-layers ${FIRST_PP_LAYERS} \\\n",
    "        --decoder-last-pipeline-num-layers ${LAST_PP_LAYERS} \\\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3ce9de",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## What scenarios are suitable for enabling Uneven pp?\n",
    "\n",
    "<span style=\"color:red\">**当不同的PP rank上计算时间不均衡时，可以使能Uneven pp, 并且这种不均衡的情况越严重，使能Uneven pp的收益越明显。**</span>\n",
    "\n",
    "当使能PP时，默认的配置，会平均分配LLM Decoder的Transformer Layers到不同的rank上。\n",
    "\n",
    "举个例子，$L_{\\text{num_transformer_layers}}=32$, `--pipeline-model-parallel-size=4`,默认每个rank上会被分配8层Transformer Layers。\n",
    "\n",
    "这个时候第一个PP rank和最后一个PP rank上会被分配更多的模块，第一个PP rank上除了8层Transformer Layers外，还会有ViT Encoder和MLP Adaptor,最后一个rank上除了8层Transformer Layers外，还有LLM Head. \n",
    "\n",
    "相比而言，平均分配LLM Decoder Tranformer Layers时，第一个pp rank上的被分配的额外模块更多，计算时间会长很多，会让其它pp rank产生更多的bubbles。\n",
    "\n",
    "为什么会产生这种情况呢？可以看如下原理的分析：\n",
    "\n",
    "<br>\n",
    "\n",
    "<span style=\"color:red\">**When the computation time is unbalanced among different PP ranks, Uneven PP can be enabled. Moreover, the more unbalanced time consumed on different ranks, the more obvious the benefits of enabling Uneven PP will be.**</span>\n",
    "\n",
    "When PP is enabled, in the default configuration, the Transformer layers of the LLM Decoder are evenly distributed across different ranks.\n",
    "\n",
    "For example, $L_{\\text{num_transformer_layers}}=32$, `--pipeline-model-parallel-size=4`, by default, each rank will be allocated 8 Transformer layers.\n",
    "\n",
    "In this case, the first and the last PP ranks will be assigned more modules. Besides the 8 Transformer layers, the first PP rank will also have a ViT Encoder and an MLP Adaptor. Similarly, in addition to the 8 Transformer layers, the last rank will have an LLM Head.\n",
    "\n",
    "In comparison, when the Transformer layers of the LLM Decoder are evenly distributed, the first PP rank is assigned more additional modules, which leads to significantly longer computation time and causes more bubbles on other PP ranks.\n",
    "\n",
    "Why does this happen? You can refer to the following analysis of the principle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903f9b02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "321e77e4",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "## Principle: Why does GPU utilization decrease when the computation time on different PP ranks is uneven?\n",
    "\n",
    "<br>\n",
    "\n",
    "REF: [Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM](https://arxiv.org/abs/2104.04473)\n",
    "\n",
    "![image.png](./images/pp_1f1b.png)\n",
    "\n",
    "We denote the number of microbatches in a batch as $m$, the number of pipeline stages (number of devices used for pipeline parallelism) as $p$, the ideal time per iteration as $t_{id}$ (assuming perfect or ideal scaling), and the time to execute a single microbatch’s forward and backward pass as $t_{f}$ and $t_{b}$. In this schedule, the pipeline bubble consists of $p - 1$ forward passes at the start of a batch, and $p - 1$ backward passes at the end. The total amount of time spent in the The pipeline bubble is then $t_{pb} = (p - 1)\\cdot(t_{f}+t_{b})$. The ideal processing time for the batch is $t_{id} = m\\cdot(t_{f} + t_{b})$. Therefore, the fraction of ideal computation time spent in the pipeline bubble is:\n",
    "\n",
    "$$\\text{Bubble time fraction (pipeline bubble size)} =  \\frac{t_{pb}}{t_{id}}=\\frac{p - 1}{m}$$ \n",
    "\n",
    "<br>\n",
    "\n",
    "<span style=\"color:red\">**Ideally, the forward and backward times consumed on each PP rank are as balanced as possible, and the bubbles time can be minimized.**</span> We can take a more intuitive look at the bubble from the following figure:\n",
    "\n",
    "![image-2.png](./images/pp_GPipe.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "Let's analyze the situation of unbalanced Pipeline Parallelism (PP):\n",
    "\n",
    "![image-2.png](./images/pp_uneven_GPipe.png)\n",
    "\n",
    "\n",
    "\n",
    "![image-2.png](./images/pp_vs_uneven_pp_additional_bubbles.png)\n",
    "\n",
    "\n",
    "<span style=\"color:red\">**从上述的对比我们可以看到，由于第一个PP stage的计算时间明显比其它PP ranks长，相比理想情况，其它rank上，引入了额外的bubbles。并且第一个PP stage的计算时间越长，其它ranks上的bubbles也会约大。因此，要让各个rank上的计算时间尽可能相同或平衡。**</span>\n",
    "\n",
    "<span style=\"color:red\">**From the above comparison, we can see that since the calculation time of the first PP stage is significantly longer than that of other PP ranks, compared with the ideal situation, additional bubbles are introduced on other ranks. Moreover, the longer the calculation time of the first PP stage, the larger the bubbles on other ranks will be. Therefore, the calculation time on each rank should be made as identical or balanced as possible.**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fc32c7",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "## How to find the optimal hyperparameter settings to enable Uneven pp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e96768",
   "metadata": {},
   "source": [
    "\n",
    "<br><br>\n",
    "## TFLOPS Calculation\n",
    "\n",
    "### ViT Encoder TFLOPS\n",
    "\n",
    "Formula for the number of floating-point operations (FLOPS) in the convolutional part (forward pass) of the Vision Transformer (ViT):\n",
    "\n",
    "$\\text{FLOPS}_{\\text{conv_forward}} = 2Nhcp^{2}$\n",
    "\n",
    " \n",
    "<br>\n",
    "Where:\n",
    "\n",
    "$h$ : Hidden size\n",
    "\n",
    "$p$ : Patch size\n",
    "\n",
    "$c$ : Image input channels\n",
    "\n",
    "$N$ : Number of image tokens after patchification\n",
    "\n",
    "$l$ : Number of transformer layers\n",
    "\n",
    "\n",
    "Formula for the number of image tokens after patchification:\n",
    "\n",
    "$N = \\left\\lfloor\\frac{W + p - 1}{p}\\right\\rfloor\\times\\left\\lfloor\\frac{H + p - 1}{p}\\right\\rfloor$\n",
    "\n",
    "\n",
    "Formula for the number of floating-point operations (FLOPS) in the Transformer part (forward pass):\n",
    "\n",
    "$\\text{FLOPS}_{\\text{transformer_forward}} = (24Nh^{2}+4hN^{2})l$\n",
    "\n",
    "\n",
    "Formula for the total trillion floating-point operations per second (TFLOPS) of the ViT (forward pass + backward pass) (Note: The classifier head is ignored):\n",
    "\n",
    "$\\text{FLOPS}_{\\text{ViT_(forward+backward)}} = 3\\times((24Nh^{2}+4hN^{2})L + 2Nhcp^{2})$\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "### Adaptor/Projector/Merger TFLOPS\n",
    "\n",
    "This part mainly aims to align the features from the Vision Transformer (ViT) with the text features. It generally consists of one or two linear layers. Here, we take a single linear layer as an example to calculate the TFLOPs.\n",
    "\n",
    "\n",
    "$\\text{FLOPS}_{\\text{projector_forward}} = 2bs_{vit}h_{vit}h_{llm}$\n",
    "\n",
    "<br>\n",
    "\n",
    "### LLM Decoder TFLOPS\n",
    "\n",
    "In most case: $h_2=4h$\n",
    "\n",
    "$\\text{FLOPS}_{\\text{LLM_(forward+backward)}} = 3\\times(24Nh^{2}+4hN^{2})L$\n",
    "\n",
    "$\\text{FLOPS}_{\\text{one_layer_forward}} = 24Nh^{2}+4hN^{2}$\n",
    "\n",
    "If $h_2\\ne4h$:\n",
    "\n",
    "$\\text{FLOPS}_{\\text{one_layer_forward}} = 8Nh^{2}+4hN^{2}+4Nhh_2$\n",
    "\n",
    "$\\text{FLOPS}_{\\text{one_layer_forward_backward}} = 3\\times(8Nh^{2}+4hN^{2}+4Nhh_2)$\n",
    "\n",
    "$\\text{FLOPS}_{\\text{llm_forward_backward}} = 3\\times(8Nh^{2}+4hN^{2}+4Nhh_2)L$\n",
    "\n",
    "Where:\n",
    "\n",
    "$h$ : Hidden size\n",
    "\n",
    "$h_2$ : FFN immediate size\n",
    "\n",
    "$N$ : Sequence length includes images tokens and text tokens\n",
    "\n",
    "$l$ : Number of transformer layers\n",
    "\n",
    "<br>\n",
    "\n",
    "<span style=\"color:red\">**Note:The hyperparameters $l$, $N$, $h$ of the LLM are different from those of the ViT. To distinguish them, we will use $l_{\\text{vit}}$, $N_{\\text{vit}}$, $h_{\\text{vit}}$, $l_{\\text{llm}}$, $N_{\\text{llm}}$, $h_{\\text{llm}}$ to replace them respectively.**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5c12cb",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## Uneven pp parameters configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d172dba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n",
      "vit_tflops: 8.75254775808\n",
      "llm_tflops: 33.462090203136\n",
      "llm_tflops_onelayer: 1.195074650112\n",
      "nlayers_per_rank:\t 17.66192511792453\n"
     ]
    }
   ],
   "source": [
    "def vit_encoder_tflops_calculator(W,H,P,hidden_size, in_channels, L):\n",
    "    N = ((W+P-1)//P) * ((H+P-1)//P)\n",
    "    print(N)\n",
    "    tflops_conv_forward = 2*N*hidden_size*in_channels*(P**2)\n",
    "    tflops_transformer_forward = (24*N*(hidden_size**2) + 4*hidden_size*(N**2))*L\n",
    "    tflops=3*(tflops_conv_forward + tflops_transformer_forward)\n",
    "    return tflops/1e12\n",
    "    \n",
    "W=224\n",
    "H=224\n",
    "P=14\n",
    "hidden_size=4096\n",
    "in_channels=3\n",
    "L=28\n",
    "\n",
    "vit_tflops=vit_encoder_tflops_calculator(W,H,P, hidden_size, in_channels, L)\n",
    "print(\"vit_tflops:\", vit_tflops)\n",
    "\n",
    "\n",
    "def llm_encoder_tflops_calculator(hidden_size, L, seq_len, intermediate_size=None):\n",
    "    if intermediate_size is None:\n",
    "        tflops_one_transformer_layer_forward=24*seq_len*(hidden_size**2) + 4*hidden_size*(seq_len**2)\n",
    "    else:\n",
    "        tflops_one_transformer_layer_forward=8*seq_len*(hidden_size**2) + 4*hidden_size*(seq_len**2) + 4*seq_len*hidden_size*intermediate_size\n",
    "#     tflops_transformer_forward = (24*seq_len*(hidden_size**2) + 4*hidden_size*(seq_len**2))*L\n",
    "    tflops_transformer_forward = tflops_one_transformer_layer_forward * L\n",
    "    tflops = tflops_transformer_forward*3/1e12\n",
    "    tflops_one_layer = tflops_one_transformer_layer_forward*3/1e12\n",
    "    return tflops, tflops_one_layer\n",
    "\n",
    "hidden_size=3584\n",
    "intermediate_size=18944\n",
    "seq_len=1024\n",
    "L=28\n",
    "\n",
    "llm_tflops, llm_tflops_onelayer=llm_encoder_tflops_calculator(hidden_size, L, seq_len,intermediate_size)\n",
    "print('llm_tflops:', llm_tflops)\n",
    "print('llm_tflops_onelayer:', llm_tflops_onelayer)\n",
    "\n",
    "nGPUs=2\n",
    "\n",
    "total_tflops = (vit_tflops+llm_tflops)\n",
    "nlayers_per_rank = total_tflops/nGPUs/llm_tflops_onelayer\n",
    "print(\"nlayers_per_rank:\\t\", nlayers_per_rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6a19a0",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "# GPU Memory Analysis (LLM)\n",
    "\n",
    "\n",
    "### Parameters\n",
    "\n",
    "Ref:[weights memory](https://nvidia-my.sharepoint.com/:p:/p/xueh/EQ8ZCgcmONpCjfMw9YZcBfsB31BAFcL1pX9oMS_DAIKn9A?e=keCH0v)\n",
    "\n",
    "$Prameters_{\\text{weights_per_transformer_layer}}=2h+12\\frac{h^2}{tp}+4h+\\frac{7h}{tp}=6h+12\\frac{h^2}{tp}+\\frac{7h}{tp}$\n",
    "\n",
    "$Parameters_{\\text{vocab}}=Vh$\n",
    "\n",
    "where:\n",
    "\n",
    "$tp$: tensor parallel size\n",
    "\n",
    "$h$: hidden size\n",
    "\n",
    "here, \n",
    "\n",
    "$h_{\\text{intermediate_size}}=4h$\n",
    "\n",
    "<br>\n",
    "\n",
    "### Total static memory per transformer layer\n",
    "\n",
    "Ref: [total memory](https://nvidia-my.sharepoint.com/:p:/p/xueh/EQ8ZCgcmONpCjfMw9YZcBfsB31BAFcL1pX9oMS_DAIKn9A?e=3DIAlm)\n",
    "\n",
    "Ref: [ZeRO](https://arxiv.org/pdf/1910.02054)\n",
    "\n",
    "If training with BF16, total static memory occupation, grad in 16bit precision:\n",
    "\n",
    "Static memory occupation:\n",
    "\n",
    "$M_{\\text{weights}}=2\\phi$\n",
    "\n",
    "$M_{\\text{grads}}=2\\phi$\n",
    "\n",
    "$M_{\\text{os}}=12\\phi$\n",
    "\n",
    "$M_{\\text{total}}=16\\phi$\n",
    "\n",
    "where:\n",
    "\n",
    "$\\phi$: parameters number\n",
    "\n",
    "So, the total static memory per transformer layer is: <span style=\"color:red\">$M_{\\text{static_memory_per_transformer_layer}}=16\\phi=16*(6h+12\\frac{h^2}{tp}+\\frac{7h}{tp})$</span>\n",
    "\n",
    "### Activation memoy per transformer layer\n",
    "\n",
    "<span style=\"color:red\">$M_{\\text{activation_memory_per_transformer_layer}}=\\frac{34bsh}{tp}$</span>\n",
    "\n",
    "To simplify the calculation, we ingore the activations of vocabulary embedding at the first stage and the LLM Head at the last stage.\n",
    "\n",
    "\n",
    "### Total memory per transformer layer\n",
    "\n",
    "$M_{\\text{per_transformer_layer}}=M_{\\text{static_memory_per_transformer_layer}} + M_{\\text{activation_memory_per_transformer_layer}}\n",
    "\n",
    "<span style=\"color:red\">$M_{\\text{per_transformer_layer}}=16*(6h+12\\frac{h^2}{tp}+\\frac{7h}{tp})+\\frac{34bsh}{tp}$</span>\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "# GPU Memory Analysis (ViT Encoder)\n",
    "\n",
    "### Parameters (ViT Encoder)\n",
    "\n",
    "$Parameters_{\\text{conv}}=p^2C_{in}C_{out}=p^2ch$\n",
    "\n",
    "$Prameters_{\\text{weights_all_transformer_layers}}=(6h+12\\frac{h^2}{tp}+\\frac{7h}{tp})l$\n",
    "\n",
    "$\\phi=p^2ch+(6h+12\\frac{h^2}{tp}+\\frac{7h}{tp})l$\n",
    "\n",
    "where:\n",
    "\n",
    "$h$ : Hidden size\n",
    "\n",
    "$p$ : Patch size\n",
    "\n",
    "$c$ : Image input channels\n",
    "\n",
    "$N$ : Number of image tokens after patchification\n",
    "\n",
    "$l$ : Number of transformer layers\n",
    "\n",
    "$\\phi$ : Parameters of ViT Encoder\n",
    "\n",
    "### Total static memory (ViT Encoder)\n",
    "\n",
    "$M_{\\text{static_memory_vit_encoder}}=16\\phi=16*(p^2ch+(6h+12\\frac{h^2}{tp}+\\frac{7h}{tp})l)$\n",
    "\n",
    "### Activation memory (ViT Encoder)\n",
    "\n",
    "$M_{\\text{activtion_memory_vit_encoder}}=xxx+\\frac{34bshl}{tp}$\n",
    "\n",
    "### Total memory (ViT Encoder)\n",
    "\n",
    "$M_{\\text{total_vit_encoder}}=M_{\\text{static_memory_vit_encoder}}+M_{\\text{activtion_memory_vit_encoder}}$\n",
    "\n",
    "<span style=\"color:red\">$M_{\\text{total_vit_encoder}}=16*(p^2ch+(6h+12\\frac{h^2}{tp}+\\frac{7h}{tp})l+xxx+\\frac{34bshl}{tp}$ </span>\n",
    "\n",
    "\n",
    "\n",
    "# GPU Memory Analysis (MLP Adaptor/projector/merger)\n",
    "\n",
    "This part mainly aims to align the features from the Vision Transformer (ViT) with the text features. It generally consists of one or two linear layers. Here, we take a single linear layer as an example to analyze the GPU memory usage.\n",
    "\n",
    "### Parameters (Linear projector)\n",
    "\n",
    "$Parameters_{\\text{projector}}=h_{\\text{vit}}h_{\\text{llm}}$\n",
    "\n",
    "### Total static memory (Linear projector)\n",
    "\n",
    "$M_{\\text{static_memory_projector}}=16\\phi=16h_{\\text{vit}}h_{\\text{llm}}$\n",
    "\n",
    "### Activation memory (Linear projector)\n",
    "\n",
    "$M_{\\text{activtion_memory_projector}}=2b{s_{\\text{vit}}}{h_{\\text{vit}}}$\n",
    "\n",
    "### Total memory (Linear projector)\n",
    "\n",
    "$M_{\\text{total_projector}}=M_{\\text{static_memory_projector}}+M_{\\text{activtion_memory_projector}}$\n",
    "\n",
    "<span style=\"color:red\">$M_{\\text{total_projector}}=16h_{\\text{vit}}h_{\\text{llm}}+2b{s_{\\text{vit}}}{h_{\\text{vit}}}$ </span>\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "# GPU memory on different pp ranks\n",
    "\n",
    "Note: For the ViT Encoder and the LLM Decoder, different settings are used for \n",
    "$h$ and $s$. To make a distinction, we will replace the original $h$ and $s$ with $h_{vit}$ and $s_{vit}$ respectively.\n",
    "\n",
    "The first pp rank:\n",
    "\n",
    "$M_{\\text{total_first_pp_rank}}=M_{\\text{total_vit_encoder}}+M_{\\text{total_projector}}+M_{\\text{total_llm_on_first_rank}}$\n",
    "\n",
    "<span style=\"color:red\">$M_{\\text{total_first_pp_rank}}=16(p^2ch_{\\text{vit}}+(6h_{\\text{vit}}+12\\frac{h_{\\text{vit}}^2}{tp}+\\frac{7h_{\\text{vit}}}{tp})l_{\\text{vit}}+xxx+\\frac{34bs_{\\text{vit}}h_{\\text{vit}}l_{\\text{vit}}}{tp} \\\\ \\hspace{4cm} +  16h_{\\text{vit}}h_{\\text{llm}}+2b{s_{\\text{vit}}}{h_{\\text{vit}}} \\\\ \\hspace{4cm} + (16(6h_{\\text{llm}}+12\\frac{h_{\\text{llm}}^2}{tp}+\\frac{7h_{\\text{llm}}}{tp})+\\frac{34bs_{\\text{llm}}h_{\\text{llm}}}{tp})l_{\\text{decoder_first_pipeline_num_layers}}$<span style=\"color:red\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "219a5946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: 6.70629888\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4221566976"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def weights_memory_calculation(hidden_size, num_layers, vocab_size=32000, intermediate_size=None):\n",
    "    h=hidden_size\n",
    "    l=num_layers\n",
    "    if intermediate_size is None:\n",
    "        parameters = (6*h + 12*(h**2) + 7*h)*l + 2*vocab_size*h\n",
    "    return parameters/1e9\n",
    "\n",
    "hidden_size=4096\n",
    "num_layers=32\n",
    "\n",
    "weights=weights_memory_calculation(hidden_size, num_layers)\n",
    "\n",
    "print('weights:', weights)\n",
    "\n",
    "\n",
    "H=4096\n",
    "L=32\n",
    "n=32\n",
    "d=128\n",
    "I=11008\n",
    "V=32000\n",
    "\n",
    "\n",
    "V*H+L*(H*H+n*d*H+2*H*I)+H*V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "45f6e466",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4758437888"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32000*4096+32*(4096*4096+2*32*128*4096+2*4096*11008)+4096*32000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd9e15f",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## How to config Uneven PP parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba16d4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
