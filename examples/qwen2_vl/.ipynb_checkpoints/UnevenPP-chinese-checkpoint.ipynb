{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6ee1bf1",
   "metadata": {},
   "source": [
    "## Uneven pp\n",
    "\n",
    "### args:\n",
    "\n",
    "```\n",
    "        --tensor-model-parallel-size ${TP} \\\n",
    "        --pipeline-model-parallel-size ${PP} \\\n",
    "        --decoder-first-pipeline-num-layers ${FIRST_PP_LAYERS} \\\n",
    "        --decoder-last-pipeline-num-layers ${LAST_PP_LAYERS} \\\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321e77e4",
   "metadata": {},
   "source": [
    "REF: [Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM](https://arxiv.org/abs/2104.04473)\n",
    "\n",
    "![image.png](./images/pp_1f1b.png)\n",
    "\n",
    "We denote the number of microbatches in a batch as $m$, the number of pipeline stages (number of devices used for pipeline parallelism) as $p$, the ideal time per iteration as $t_{id}$ (assuming perfect or ideal scaling), and the time to execute a single microbatchâ€™s forward and backward pass as $t_{f}$ and $t_{b}$. In this schedule, the pipeline bubble consists of $p - 1$ forward passes at the start of a batch, and $p - 1$ backward passes at the end. The total amount of time spent in the The pipeline bubble is then $t_{pb} = (p - 1)\\cdot(t_{f}+t_{b})$. The ideal processing time for the batch is $t_{id} = m\\cdot(t_{f} + t_{b})$. Therefore, the fraction of ideal computation time spent in the pipeline bubble is:\n",
    "\n",
    "$$\\text{Bubble time fraction (pipeline bubble size)} =  \\frac{t_{pb}}{t_{id}}=\\frac{p - 1}{m}$$ \n",
    "\n",
    "<br>\n",
    "\n",
    "Ideally, the forward and reverse times on each pipeline rank are as balanced as possible, and the bubble time can be minimized. We can take a more intuitive look at the bubble from the following figure:\n",
    "\n",
    "![image-2.png](./images/pp_GPipe.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "Let's analyze the situation of unbalanced Pipeline Parallelism (PP):\n",
    "\n",
    "![image-2.png](./images/pp_uneven_GPipe.png)\n",
    "\n",
    "\n",
    "\n",
    "![image-2.png](./images/pp_vs_uneven_pp_additional_bubbles.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e96768",
   "metadata": {},
   "source": [
    "\n",
    "<br><br>\n",
    "## TFLOPS Calculation\n",
    "\n",
    "### ViT Encoder TFLOPS\n",
    "\n",
    "Formula for the number of floating-point operations (FLOPS) in the convolutional part (forward pass) of the Vision Transformer (ViT):\n",
    "\n",
    "$\\text{FLOPS}_{\\text{conv_forward}} = 2Nhcp^{2}$\n",
    "\n",
    " \n",
    "<br>\n",
    "Where:\n",
    "\n",
    "$h$ : Hidden size\n",
    "\n",
    "$p$ : Patch size\n",
    "\n",
    "$c$ : Image input channels\n",
    "\n",
    "$N$ : Number of image tokens after patchification\n",
    "\n",
    "$l$ : Number of transformer layers\n",
    "\n",
    "\n",
    "Formula for the number of image tokens after patchification:\n",
    "\n",
    "$N = \\left\\lfloor\\frac{W + p - 1}{p}\\right\\rfloor\\times\\left\\lfloor\\frac{H + p - 1}{p}\\right\\rfloor$\n",
    "\n",
    "\n",
    "Formula for the number of floating-point operations (FLOPS) in the Transformer part (forward pass):\n",
    "\n",
    "$\\text{FLOPS}_{\\text{transformer_forward}} = (24Nh^{2}+4hN^{2})l$\n",
    "\n",
    "\n",
    "Formula for the total trillion floating-point operations per second (TFLOPS) of the ViT (forward pass + backward pass) (Note: The classifier head is ignored):\n",
    "\n",
    "$\\text{FLOPS}_{\\text{ViT_(forward+backward)}} = 3\\times((24Nh^{2}+4hN^{2})L + 2Nhcp^{2})$\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "### LLM TFLOPS\n",
    "\n",
    "In most case: $h_2=4h$\n",
    "\n",
    "$\\text{FLOPS}_{\\text{LLM_(forward+backward)}} = 3\\times(24Nh^{2}+4hN^{2})L$\n",
    "\n",
    "$\\text{FLOPS}_{\\text{one_layer_forward}} = 24Nh^{2}+4hN^{2}$\n",
    "\n",
    "If $h_2\\ne4h$:\n",
    "\n",
    "$\\text{FLOPS}_{\\text{one_layer_forward}} = 8Nh^{2}+4hN^{2}+4Nhh_2$\n",
    "\n",
    "$\\text{FLOPS}_{\\text{one_layer_forward_backward}} = 3\\times(8Nh^{2}+4hN^{2}+4Nhh_2)$\n",
    "\n",
    "$\\text{FLOPS}_{\\text{llm_forward_backward}} = 3\\times(8Nh^{2}+4hN^{2}+4Nhh_2)L$\n",
    "\n",
    "Where:\n",
    "\n",
    "$h$ : Hidden size\n",
    "\n",
    "$h_2$ : FFN immediate size\n",
    "\n",
    "$N$ : Sequence length includes images tokens and text tokens\n",
    "\n",
    "$l$ : Number of transformer layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5c12cb",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## Uneven pp parameters configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d172dba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n",
      "vit_tflops: 8.75254775808\n",
      "llm_tflops: 33.462090203136\n",
      "llm_tflops_onelayer: 1.195074650112\n",
      "nlayers_per_rank:\t 17.66192511792453\n"
     ]
    }
   ],
   "source": [
    "def vit_encoder_tflops_calculator(W,H,P,hidden_size, in_channels, L):\n",
    "    N = ((W+P-1)//P) * ((H+P-1)//P)\n",
    "    print(N)\n",
    "    tflops_conv_forward = 2*N*hidden_size*in_channels*(P**2)\n",
    "    tflops_transformer_forward = (24*N*(hidden_size**2) + 4*hidden_size*(N**2))*L\n",
    "    tflops=3*(tflops_conv_forward + tflops_transformer_forward)\n",
    "    return tflops/1e12\n",
    "    \n",
    "W=224\n",
    "H=224\n",
    "P=14\n",
    "hidden_size=4096\n",
    "in_channels=3\n",
    "L=28\n",
    "\n",
    "vit_tflops=vit_encoder_tflops_calculator(W,H,P, hidden_size, in_channels, L)\n",
    "print(\"vit_tflops:\", vit_tflops)\n",
    "\n",
    "\n",
    "def llm_encoder_tflops_calculator(hidden_size, L, seq_len, intermediate_size=None):\n",
    "    if intermediate_size is None:\n",
    "        tflops_one_transformer_layer_forward=24*seq_len*(hidden_size**2) + 4*hidden_size*(seq_len**2)\n",
    "    else:\n",
    "        tflops_one_transformer_layer_forward=8*seq_len*(hidden_size**2) + 4*hidden_size*(seq_len**2) + 4*seq_len*hidden_size*intermediate_size\n",
    "#     tflops_transformer_forward = (24*seq_len*(hidden_size**2) + 4*hidden_size*(seq_len**2))*L\n",
    "    tflops_transformer_forward = tflops_one_transformer_layer_forward * L\n",
    "    tflops = tflops_transformer_forward*3/1e12\n",
    "    tflops_one_layer = tflops_one_transformer_layer_forward*3/1e12\n",
    "    return tflops, tflops_one_layer\n",
    "\n",
    "hidden_size=3584\n",
    "intermediate_size=18944\n",
    "seq_len=1024\n",
    "L=28\n",
    "\n",
    "llm_tflops, llm_tflops_onelayer=llm_encoder_tflops_calculator(hidden_size, L, seq_len,intermediate_size)\n",
    "print('llm_tflops:', llm_tflops)\n",
    "print('llm_tflops_onelayer:', llm_tflops_onelayer)\n",
    "\n",
    "nGPUs=2\n",
    "\n",
    "total_tflops = (vit_tflops+llm_tflops)\n",
    "nlayers_per_rank = total_tflops/nGPUs/llm_tflops_onelayer\n",
    "print(\"nlayers_per_rank:\\t\", nlayers_per_rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6a19a0",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "# GPU Memory Analysis (LLM)\n",
    "\n",
    "\n",
    "### Parameters\n",
    "\n",
    "Ref:[weights memory](https://nvidia-my.sharepoint.com/:p:/p/xueh/EQ8ZCgcmONpCjfMw9YZcBfsB31BAFcL1pX9oMS_DAIKn9A?e=keCH0v)\n",
    "\n",
    "$Prameters_{\\text{weights_per_transformer_layer}}=2h+12\\frac{h^2}{tp}+4h+\\frac{7h}{tp}=6h+12\\frac{h^2}{tp}+\\frac{7h}{tp}$\n",
    "\n",
    "$Parameters_{\\text{vocab}}=Vh$\n",
    "\n",
    "where:\n",
    "\n",
    "$tp$: tensor parallel size\n",
    "\n",
    "$h$: hidden size\n",
    "\n",
    "here, \n",
    "\n",
    "$h_{\\text{intermediate_size}}=4h$\n",
    "\n",
    "<br>\n",
    "\n",
    "### Total static memory per transformer layer\n",
    "\n",
    "Ref: [total memory](https://nvidia-my.sharepoint.com/:p:/p/xueh/EQ8ZCgcmONpCjfMw9YZcBfsB31BAFcL1pX9oMS_DAIKn9A?e=3DIAlm)\n",
    "\n",
    "Ref: [ZeRO](https://arxiv.org/pdf/1910.02054)\n",
    "\n",
    "If training with BF16, total static memory occupation, grad in 16bit precision:\n",
    "\n",
    "Static memory occupation:\n",
    "\n",
    "$M_{\\text{weights}}=2\\phi$\n",
    "\n",
    "$M_{\\text{grads}}=2\\phi$\n",
    "\n",
    "$M_{\\text{os}}=12\\phi$\n",
    "\n",
    "$M_{\\text{total}}=16\\phi$\n",
    "\n",
    "where:\n",
    "\n",
    "$\\phi$: parameters number\n",
    "\n",
    "So, the total static memory per transformer layer is: <span style=\"color:red\">$M_{\\text{static_memory_per_transformer_layer}}=16\\phi=16*(6h+12\\frac{h^2}{tp}+\\frac{7h}{tp})$</span>\n",
    "\n",
    "### Activation memoy per transformer layer\n",
    "\n",
    "<span style=\"color:red\">$M_{\\text{activation_memory_per_transformer_layer}}=\\frac{34bsh}{tp}$</span>\n",
    "\n",
    "To simplify the calculation, we ingore the activations of vocabulary embedding at the first stage and the LLM Head at the last stage.\n",
    "\n",
    "\n",
    "### Total memory per transformer layer\n",
    "\n",
    "<span style=\"color:red\">$M_{\\text{per_transformer_layer}}=M_{\\text{static_memory_per_transformer_layer}} + M_{\\text{activation_memory_per_transformer_layer}}$</span>\n",
    "\n",
    "<span style=\"color:red\">$M_{\\text{per_transformer_layer}}=16*(6h+12\\frac{h^2}{tp}+\\frac{7h}{tp})+\\frac{34bsh}{tp}$</span>\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "# GPU Memory Analysis (ViT Encoder)\n",
    "\n",
    "### Parameters (ViT Encoder)\n",
    "\n",
    "$Parameters_{\\text{conv}}=p^2C_{in}C_{out}=p^2ch$\n",
    "\n",
    "$Prameters_{\\text{weights_all_transformer_layers}}=(6h+12\\frac{h^2}{tp}+\\frac{7h}{tp})l$\n",
    "\n",
    "$\\phi=p^2ch+(6h+12\\frac{h^2}{tp}+\\frac{7h}{tp})l$\n",
    "\n",
    "where:\n",
    "\n",
    "$h$ : Hidden size\n",
    "\n",
    "$p$ : Patch size\n",
    "\n",
    "$c$ : Image input channels\n",
    "\n",
    "$N$ : Number of image tokens after patchification\n",
    "\n",
    "$l$ : Number of transformer layers\n",
    "\n",
    "$\\phi$ : Parameters of ViT Encoder\n",
    "\n",
    "### Total static memory (ViT Encoder)\n",
    "\n",
    "$M_{\\text{static_memory_vit_encoder}}=16\\phi=16*(p^2ch+(6h+12\\frac{h^2}{tp}+\\frac{7h}{tp})l)$\n",
    "\n",
    "### Activation memory (ViT Encoder)\n",
    "\n",
    "$M_{\\text{activtion_memory_vit_encoder}}=xxx+\\frac{34bshl}{tp}$\n",
    "\n",
    "### Total memory (ViT Encoder)\n",
    "\n",
    "$M_{\\text{total_vit_encoder}}=M_{\\text{static_memory_vit_encoder}}+M_{\\text{activtion_memory_vit_encoder}}$\n",
    "\n",
    "<span style=\"color:red\">$M_{\\text{total_vit_encoder}}=16*(p^2ch+(6h+12\\frac{h^2}{tp}+\\frac{7h}{tp})l+xxx+\\frac{34bshl}{tp}$ </span>\n",
    "\n",
    "\n",
    "\n",
    "# GPU Memory Analysis (MLP Adaptor/projector/merger)\n",
    "\n",
    "This part mainly aims to align the features from the Vision Transformer (ViT) with the text features. It generally consists of one or two linear layers. Here, we take a single linear layer as an example to analyze the GPU memory usage.\n",
    "\n",
    "### Parameters (MLP/Linear projector)\n",
    "\n",
    "$Parameters_{\\text{projector}}=h_{\\text{vit}}h_{\\text{llm}}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "219a5946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: 6.70629888\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4221566976"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def weights_memory_calculation(hidden_size, num_layers, vocab_size=32000, intermediate_size=None):\n",
    "    h=hidden_size\n",
    "    l=num_layers\n",
    "    if intermediate_size is None:\n",
    "        parameters = (6*h + 12*(h**2) + 7*h)*l + 2*vocab_size*h\n",
    "    return parameters/1e9\n",
    "\n",
    "hidden_size=4096\n",
    "num_layers=32\n",
    "\n",
    "weights=weights_memory_calculation(hidden_size, num_layers)\n",
    "\n",
    "print('weights:', weights)\n",
    "\n",
    "\n",
    "H=4096\n",
    "L=32\n",
    "n=32\n",
    "d=128\n",
    "I=11008\n",
    "V=32000\n",
    "\n",
    "\n",
    "V*H+L*(H*H+n*d*H+2*H*I)+H*V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "45f6e466",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4758437888"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32000*4096+32*(4096*4096+2*32*128*4096+2*4096*11008)+4096*32000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd9e15f",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## How to config Uneven PP parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba16d4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
