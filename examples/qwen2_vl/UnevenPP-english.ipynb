{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6ee1bf1",
   "metadata": {},
   "source": [
    "# Uneven pp\n",
    "\n",
    "## Parameters involved in the configuration file:\n",
    "\n",
    "```\n",
    "        --tensor-model-parallel-size ${TP} \\\n",
    "        --pipeline-model-parallel-size ${PP} \\\n",
    "        --decoder-first-pipeline-num-layers ${FIRST_PP_LAYERS} \\\n",
    "        --decoder-last-pipeline-num-layers ${LAST_PP_LAYERS} \\\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d18f43",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## What scenarios are suitable for enabling Uneven pp?\n",
    "\n",
    "<span style=\"color:red\">**When the computation time is unbalanced among different PP ranks, Uneven PP can be enabled. Moreover, the more unbalanced time consumed on different ranks, the more obvious the benefits of enabling Uneven PP will be.**</span>\n",
    "\n",
    "When PP is enabled, in the default configuration, the Transformer layers of the LLM Decoder are evenly distributed across different ranks.\n",
    "\n",
    "For example, $L_{\\text{num_transformer_layers}}=32$, `--pipeline-model-parallel-size=4`, by default, each rank will be allocated 8 Transformer layers.\n",
    "\n",
    "In this case, the first and the last PP ranks will be assigned more modules. Besides the 8 Transformer layers, the first PP rank will also have a ViT Encoder and an MLP Adaptor. Similarly, in addition to the 8 Transformer layers, the last rank will have an LLM Head.\n",
    "\n",
    "In comparison, when the Transformer layers of the LLM Decoder are evenly distributed, the first PP rank is assigned more additional modules, which leads to significantly longer computation time and causes more bubbles on other PP ranks.\n",
    "\n",
    "Why does this happen? You can refer to the following analysis of the principle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22865618",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "321e77e4",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "## Principle: Why does GPU utilization decrease when the computation time on different PP ranks is uneven?\n",
    "\n",
    "<br>\n",
    "\n",
    "REF: [Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM](https://arxiv.org/abs/2104.04473)\n",
    "\n",
    "![image.png](./images/pp_1f1b.png)\n",
    "\n",
    "We denote the number of microbatches in a batch as $m$, the number of pipeline stages (number of devices used for pipeline parallelism) as $p$, the ideal time per iteration as $t_{id}$ (assuming perfect or ideal scaling), and the time to execute a single microbatch’s forward and backward pass as $t_{f}$ and $t_{b}$. In this schedule, the pipeline bubble consists of $p - 1$ forward passes at the start of a batch, and $p - 1$ backward passes at the end. The total amount of time spent in the The pipeline bubble is then $t_{pb} = (p - 1)\\cdot(t_{f}+t_{b})$. The ideal processing time for the batch is $t_{id} = m\\cdot(t_{f} + t_{b})$. Therefore, the fraction of ideal computation time spent in the pipeline bubble is:\n",
    "\n",
    "$$\\text{Bubble time fraction (pipeline bubble size)} =  \\frac{t_{pb}}{t_{id}}=\\frac{p - 1}{m}$$ \n",
    "\n",
    "<br>\n",
    "\n",
    "<span style=\"color:red\">**Ideally, the forward and backward times consumed on each PP rank are as balanced as possible, and the bubbles time can be minimized.**</span> We can take a more intuitive look at the bubble from the following figure:\n",
    "\n",
    "![image-2.png](./images/pp_GPipe.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "Let's analyze the situation of unbalanced Pipeline Parallelism (PP):\n",
    "\n",
    "![image-2.png](./images/pp_uneven_GPipe.png)\n",
    "\n",
    "\n",
    "\n",
    "![image-2.png](./images/pp_vs_uneven_pp_additional_bubbles.png)\n",
    "\n",
    "\n",
    "<span style=\"color:red\">**From the above comparison, we can see that since the calculation time of the first PP stage is significantly longer than that of other PP ranks, compared with the ideal situation, additional bubbles are introduced on other ranks. Moreover, the longer the calculation time of the first PP stage, the larger the bubbles on other ranks will be. Therefore, the calculation time on each rank should be made as identical or balanced as possible.**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3902d348",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "## How to find the optimal hyperparameter settings to enable Uneven pp\n",
    "\n",
    "We need to make the calculation time on each pipeline parallel (PP) rank as balanced as possible. Before the formal training run, we can preliminarily estimate the GPU calculation time through TeraFLOPS (tflops). Therefore, making the calculation time on each PP rank as balanced as possible can be simplified to making the tflops of the modules on each PP rank as balanced as possible.\n",
    "\n",
    "Note: For the first PP rank, the calculation time includes not only the calculation time of the model but also the data loading time. This is a non - negligible part of the time when configuring the parameters of Uneven PP. However, this part of the time cannot be estimated by floating - point operations. For simplicity, we do not take this time into account during the calculation. But in the final result, we round down the number of large language models (LLMs) assigned to the first PP rank and round up the number of LLMs assigned to other ranks.\n",
    "\n",
    "<br>\n",
    "\n",
    "$TFLOPS_{\\text{total}} = TFLOPS_{\\text{vit_encoder}} + TFLOPS_{\\text{adaptor}} + TFLOPS_{\\text{llm_decoder}}$\n",
    "\n",
    "<br>\n",
    "\n",
    "For an even distribution, the TFLOPS on each pipeline parallel (PP) rank is：\n",
    "\n",
    "$TFLOPS_{\\text{per_pp_rank}}=\\frac{TFLOPS_{\\text{total}}}{\\text{pp}}$\n",
    "\n",
    "Correspondingly, the number of LLM decoder transformer layers that should be assigned to each intermediate rank is：\n",
    "\n",
    "$L_{\\text{per_middle_pp_rank}} = \\lceil{\\frac{TFLOPS_{\\text{per_pp_rank}}}{TFLOPS_{\\text{per_llm_decoder_one_transformer_layer}}}}\\rceil$\n",
    "\n",
    "<br>\n",
    "\n",
    "Therefore, the most important parameter in Uneven PP, `--decoder-first-pipeline-num-layers`:\n",
    "\n",
    "$L_{\\text{first_decoder_pp_rank}}=Layers_{\\text{llm_decoder}}-L_{\\text{per_middle_pp_rank}}*(pp-1)$ \n",
    "\n",
    "<br>\n",
    "\n",
    "The parameters mentioned above are explained as follows：\n",
    "\n",
    "$pp$: pipeline-model-parallel-siz\n",
    "\n",
    "$Layers_{\\text{llm_decoder}}$: total number of llm decoder transformer layers\n",
    "\n",
    "$L_{\\text{per_middle_pp_rank}}$: number of transformer layers allocated on each pipeline rank\n",
    "\n",
    "$L_{\\text{first_decoder_pp_rank}}$: decoder-first-pipeline-num-layers\n",
    "\n",
    "For the detailed calculation process of TFLOPS, you can refer to：[TFLOPS Calculation Part](http://10.23.206.92:7888/notebooks/UnevenPP-chinese.ipynb#TFLOPS-Calculation)\n",
    "\n",
    "<br>\n",
    "\n",
    "To sum up, we can calculate `--decoder-first-pipeline-num-layers` as follows：\n",
    "\n",
    "\n",
    "$L_{\\text{first_decoder_pp_rank}}=Layers_{\\text{llm_decoder}}-L_{\\text{per_middle_pp_rank}}*(pp-1)$ \n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "<span style=\"color:red\">$L_{\\text{first_decoder_pp_rank}}=Layers_{\\text{llm_decoder}}-{\\frac{TFLOPS_{\\text{total}}}{\\text{pp}}}*(pp-1)$ </span>\n",
    "\n",
    "<br>\n",
    "\n",
    "For example, here's how to set the hyperparameters of Uneven PP under different hyperparameter scenarios：\n",
    "1. Case1: General Vision Transformer (ViT) settings\n",
    "2. Case2: To better observe the advantageous scenarios of Uneven PP, change the $\\text{vit_hidden_size}$ from 1280 to 4096.\n",
    "    * In this case, the `--decoder-first-pipeline-num-layers` should be assigned fewer transformer layers.\n",
    "3. The proportion of TFLOPS in the ViT Encoder part exceeds the TFLOPS that would be evenly distributed to each pipeline parallel (PP) rank.\n",
    "    * In this case, set `--decoder-first-pipeline-num-layers=0`, and the transformer layers of the large language model (LLM) are distributed among the remaining PP ranks.\n",
    "    \n",
    "<br>\n",
    "\n",
    "\n",
    "You can use the `uneven_pp_parameters_generator` function in [uneven_pp_utils.py](./uneven_pp_utils.py) to quickly estimate the hyperparameter configuration:\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2b3350f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "case1 (vit ~500M):\n",
      "num_layers_first_pp_rank: 13\n",
      "num_layers_last_pp_rank: 15\n",
      "\n",
      "case2: (vit ~5.6G)\n",
      "num_layers_first_pp_rank: 10\n",
      "num_layers_last_pp_rank: 18\n",
      "\n",
      "case3: (vit ~21G)\n",
      "num_layers_first_pp_rank: 0\n",
      "num_layers_last_pp_rank: 28\n"
     ]
    }
   ],
   "source": [
    "from uneven_pp_utils import *\n",
    "\n",
    "# *********************************************************************************************\n",
    "# case 1\n",
    "# vit encoder parameters\n",
    "image_w = 224\n",
    "image_h = 224\n",
    "image_in_channels = 3\n",
    "patch_size = 14\n",
    "vit_hidden_size = 1280\n",
    "vit_intermediate_size = vit_hidden_size*4\n",
    "vit_num_layers = 28\n",
    "\n",
    "# adaptor parameters\n",
    "# vit_hidden_size = 1280\n",
    "# llm_hidden_size = 3584\n",
    "\n",
    "# llm decoder parameters\n",
    "decoder_seq_len = 1024\n",
    "llm_hidden_size = 3584\n",
    "llm_intermediate_size = 18944\n",
    "llm_num_layers = 28\n",
    "\n",
    "pp_size = 2\n",
    "\n",
    "num_layers_first_pp_rank, num_layers_last_pp_rank = uneven_pp_parameters_generator(\n",
    "    image_w, image_h, image_in_channels, patch_size, vit_num_layers, vit_hidden_size, vit_intermediate_size, \n",
    "    decoder_seq_len, llm_num_layers, llm_hidden_size, llm_intermediate_size, pp_size)\n",
    "\n",
    "print('\\ncase1 (vit ~500M):')\n",
    "print('num_layers_first_pp_rank:', num_layers_first_pp_rank)\n",
    "print('num_layers_last_pp_rank:', num_layers_last_pp_rank)\n",
    "\n",
    "# *********************************************************************************************\n",
    "# case 2\n",
    "\n",
    "# vit encoder parameters\n",
    "image_w = 224\n",
    "image_h = 224\n",
    "image_in_channels = 3\n",
    "patch_size = 14\n",
    "vit_hidden_size = 4096\n",
    "vit_intermediate_size = vit_hidden_size*4\n",
    "vit_num_layers = 28\n",
    "\n",
    "# adaptor parameters\n",
    "# vit_hidden_size = 1280\n",
    "# llm_hidden_size = 3584\n",
    "\n",
    "# llm decoder parameters\n",
    "decoder_seq_len = 1024\n",
    "llm_hidden_size = 3584\n",
    "llm_intermediate_size = 18944\n",
    "llm_num_layers = 28\n",
    "\n",
    "pp_size = 2\n",
    "\n",
    "num_layers_first_pp_rank, num_layers_last_pp_rank = uneven_pp_parameters_generator(\n",
    "    image_w, image_h, image_in_channels, patch_size, vit_num_layers, vit_hidden_size, vit_intermediate_size, \n",
    "    decoder_seq_len, llm_num_layers, llm_hidden_size, llm_intermediate_size, pp_size)\n",
    "\n",
    "print('\\ncase2: (vit ~5.6G)')\n",
    "print('num_layers_first_pp_rank:', num_layers_first_pp_rank)\n",
    "print('num_layers_last_pp_rank:', num_layers_last_pp_rank) \n",
    "\n",
    "# *********************************************************************************************\n",
    "# case 3\n",
    "# vit encoder parameters\n",
    "image_w = 224\n",
    "image_h = 224\n",
    "image_in_channels = 3\n",
    "patch_size = 14\n",
    "vit_hidden_size = 8000\n",
    "vit_intermediate_size = vit_hidden_size*4\n",
    "vit_num_layers = 28\n",
    "\n",
    "# adaptor parameters\n",
    "# vit_hidden_size = 1280\n",
    "# llm_hidden_size = 3584\n",
    "\n",
    "# llm decoder parameters\n",
    "decoder_seq_len = 1024\n",
    "llm_hidden_size = 3584\n",
    "llm_intermediate_size = 18944\n",
    "llm_num_layers = 28\n",
    "\n",
    "pp_size = 2\n",
    "\n",
    "num_layers_first_pp_rank, num_layers_last_pp_rank = uneven_pp_parameters_generator(\n",
    "    image_w, image_h, image_in_channels, patch_size, vit_num_layers, vit_hidden_size, vit_intermediate_size, \n",
    "    decoder_seq_len, llm_num_layers, llm_hidden_size, llm_intermediate_size, pp_size)\n",
    "\n",
    "print('\\ncase3: (vit ~21G)')\n",
    "print('num_layers_first_pp_rank:', num_layers_first_pp_rank)\n",
    "print('num_layers_last_pp_rank:', num_layers_last_pp_rank) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9793b8f",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "<br>\n",
    "\n",
    "## GPU memory estimating\n",
    "\n",
    "\n",
    "Estimate whether the GPU memory usage is reasonable under the current configuration. (Note: The following estimates do not take into account various memory optimization techniques, and the actual memory usage should be based on the real situation.)\n",
    "\n",
    "For the analysis of GPU memory usage of each part, you can refer to the **GPU Memory Analysis (LLM)** section.\n",
    "\n",
    "You can use the `memory_first_pp_rank` function in [uneven_pp_utils.py](./uneven_pp_utils.py) to quickly estimate whether the GPU memory usage is reasonable.\n",
    "\n",
    "If you adopt the configuration of case 2 with `pp = 2` and `tp = 1`, when estimating the memory on the first PP rank, the estimated memory is about 120GB, which exceeds the 96GB memory capacity of the H20 GPU.\n",
    "\n",
    "So, we try the configuration of case 2 again with `pp = 2` and `tp = 2`. At this time, the estimated memory on the first PP rank is about 61GB, which meets the 96GB memory capacity of the H20 GPU.\n",
    "\n",
    "Similarly, we need to verify whether the memory on other PP ranks meets the requirements. Finally, the configuration of case 2 with `pp = 2` and `tp = 2` can meet the GPU memory requirements. Therefore, we will conduct experiments with `pp = 2` and `tp = 2` next.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6394438a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "case 2: memory analysi, tp=2, pp2\n",
      "total_memory_vit_encoder:91.255 GB\n",
      "memory_llm_decoder_on_first_pp_rank:31.392 GB\n",
      "memory on first pp rank:122.647 GB\n",
      "\n",
      "case 2: memory analysi, tp=2, pp1\n",
      "total_memory_vit_encoder:45.653 GB\n",
      "memory_llm_decoder_on_first_pp_rank:15.698 GB\n",
      "memory on first pp rank:61.350 GB\n"
     ]
    }
   ],
   "source": [
    "from uneven_pp_utils import *\n",
    "from uneven_pp_utils import memory_first_pp_rank_calculator\n",
    "\n",
    "# *********************************************************************************************\n",
    "# case 2: memory analysi, tp=1, pp2\n",
    "\n",
    "print('\\ncase 2: memory analysi, tp=2, pp2')\n",
    "# vit encoder parameters\n",
    "image_w = 224\n",
    "image_h = 224\n",
    "image_in_channels = 3\n",
    "patch_size = 14\n",
    "vit_hidden_size = 4096\n",
    "vit_intermediate_size = vit_hidden_size*4\n",
    "vit_num_layers = 28\n",
    "\n",
    "bs=1\n",
    "tp=1\n",
    "\n",
    "# llm decoder parameters\n",
    "decoder_seq_len = 1024\n",
    "llm_hidden_size = 3584\n",
    "llm_intermediate_size = 18944\n",
    "llm_num_layers = 28\n",
    "\n",
    "# uneven pp\n",
    "num_layers_first_pp_stage=10\n",
    "\n",
    "memory_vit_encoder_calculator(image_w, image_h, image_in_channels, patch_size, vit_num_layers, vit_hidden_size, vit_intermediate_size, bs, tp)\n",
    "\n",
    "memory_first_pp_rank = memory_first_pp_rank_calculator(image_w, image_h, image_in_channels, patch_size, vit_num_layers, vit_hidden_size, vit_intermediate_size, decoder_seq_len, llm_num_layers, llm_hidden_size, llm_intermediate_size, num_layers_first_pp_stage, bs, tp)\n",
    "\n",
    "# *********************************************************************************************\n",
    "# case 2: memory analysi, tp=2, pp2\n",
    "\n",
    "print('\\ncase 2: memory analysi, tp=2, pp1')\n",
    "# vit encoder parameters\n",
    "image_w = 224\n",
    "image_h = 224\n",
    "image_in_channels = 3\n",
    "patch_size = 14\n",
    "vit_hidden_size = 4096\n",
    "vit_intermediate_size = vit_hidden_size*4\n",
    "vit_num_layers = 28\n",
    "\n",
    "bs=1\n",
    "tp=2\n",
    "\n",
    "# llm decoder parameters\n",
    "decoder_seq_len = 1024\n",
    "llm_hidden_size = 3584\n",
    "llm_intermediate_size = 18944\n",
    "llm_num_layers = 28\n",
    "\n",
    "# uneven pp\n",
    "num_layers_first_pp_stage=10\n",
    "\n",
    "memory_vit_encoder_calculator(image_w, image_h, image_in_channels, patch_size, vit_num_layers, vit_hidden_size, vit_intermediate_size, bs, tp)\n",
    "\n",
    "memory_first_pp_rank = memory_first_pp_rank_calculator(image_w, image_h, image_in_channels, patch_size, vit_num_layers, vit_hidden_size, vit_intermediate_size, decoder_seq_len, llm_num_layers, llm_hidden_size, llm_intermediate_size, num_layers_first_pp_stage, bs, tp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb3f421",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "****\n",
    "\n",
    "# Tips\n",
    "\n",
    "The parameter configuration of Uneven pp can be determined through the following steps:\n",
    "1. Ensure the balanced allocation of computing power. Calculate the parameter configuration of Uneven pp based on TFLOPS. For instance, in the example, set`--decoder-first-pipeline-num-layers=10`, `--pipeline-model-parallel-siz=2`, and `--tensor-model-parallel-siz=1`.\n",
    "2. Ensure that the GPU memory meets the requirements. As in the example, when setting `--decoder-first-pipeline-num-layers=10`, `--pipeline-model-parallel-siz=2`, and `--tensor-model-parallel-siz=1`, it will exceed the GPU memory capacity of the H20-96G. Therefore, the final selection is `--decoder-first-pipeline-num-layers=10`, `--pipeline-model-parallel-siz=2`, and `--tensor-model-parallel-siz=2`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1116cf",
   "metadata": {},
   "source": [
    "\n",
    "<br><br>\n",
    "\n",
    "****\n",
    "\n",
    "# Examples: Uneven pp enabled vs. Uneven pp disabled\n",
    "\n",
    "以case 2为例来看一下Uneven pp开启所能带来的性能提升：\n",
    "\n",
    "参考脚本：[run_mcore_qwen_h20.sh](./run_mcore_qwen_h20.sh)\n",
    "\n",
    "## exp1: uneven pp disabled\n",
    "\n",
    "```\n",
    "sh run_mcore_qwen_h20.sh  \\\n",
    "dsw  \\\n",
    "7B   \\\n",
    "1    \\\n",
    "32 \\\n",
    "1e-5   \\\n",
    "1e-6   \\\n",
    "1024  \\\n",
    "1024  \\\n",
    "bf16  \\\n",
    "2   \\\n",
    "2  \\\n",
    "1 \\\n",
    "false \\\n",
    "false \\\n",
    "true   \\\n",
    "false \\\n",
    "false \\\n",
    "100000  \\\n",
    "/workspace/data/mm/LLaVA-Pretrain/wds   \\\n",
    "/workspace/data/mm/LLaVA-Pretrain/wds   \\\n",
    "/workspace/data/mm/model/qwen2-vl-ckpts/Qwen2-VL-2B-Instruct \\\n",
    "20000  \\\n",
    "200   \\\n",
    "./output_mcore_qwen2vl_pretrain \\\n",
    "14 \\\n",
    "0\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "## exp1: uneven pp enabled\n",
    "\n",
    "```\n",
    "sh run_mcore_qwen_h20.sh  \\\n",
    "dsw  \\\n",
    "7B   \\\n",
    "1    \\\n",
    "32 \\\n",
    "1e-5   \\\n",
    "1e-6   \\\n",
    "1024  \\\n",
    "1024  \\\n",
    "bf16  \\\n",
    "2   \\\n",
    "2  \\\n",
    "1 \\\n",
    "false \\\n",
    "false \\\n",
    "true   \\\n",
    "false \\\n",
    "false \\\n",
    "100000  \\\n",
    "/workspace/data/mm/LLaVA-Pretrain/wds   \\\n",
    "/workspace/data/mm/LLaVA-Pretrain/wds   \\\n",
    "/workspace/data/mm/model/qwen2-vl-ckpts/Qwen2-VL-2B-Instruct \\\n",
    "20000  \\\n",
    "200   \\\n",
    "./output_mcore_qwen2vl_pretrain \\\n",
    "10 \\\n",
    "0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260ee8c8",
   "metadata": {},
   "source": [
    "<br>\n",
    "使能Uneven pp时，获得的加速效果如下：\n",
    "\n",
    "# Benchmark: Uneven pp enabled v.s. Uneven pp disabled on case 2\n",
    "\n",
    "|config|Time per iteration (ms)|Speedup|GPU|mbs/gbs|\n",
    "|:-:|:-:|:-:|:-:|:-:|\n",
    "|Uneven pp disabled|28388.7||H20|1/32|\n",
    "|Uneven pp enabled （10，18）|19607.1|1.448|H20|1/32|\n",
    "\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bdc82b",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "***\n",
    "\n",
    "# Timeline (Uneven pp disabled vs. Uneven pp enabled)\n",
    "\n",
    "通过如下的timeline我看可以清楚的看到：\n",
    "1. 当Uneven pp使能时 （`--decoder-first-pipeline-num-layers=12`， `--decoder-last-pipeline-num-layers=16`, `--pipeline-model-parallel-size=2`），两个PP ranks之间的算力分配更加均衡 （32.9% vs. 34.1%）；\n",
    "2. 当Uneven pp不使能时 （`--decoder-first-pipeline-num-layers=14`， `--decoder-last-pipeline-num-layers=14`, `--pipeline-model-parallel-size=2`），两个PP ranks之间的算力分配不均衡 (34.1% vs. 17.4)；\n",
    "3. 对比Uneven pp使能和不使能时，会发现当Uneven pp不使能时，pp rank1上通信等待的时间会更长，有更多的通信等待bubbles.\n",
    "\n",
    "<br>\n",
    "\n",
    "![](./images/nsys_timeline_even_pp_vs_uneven_pp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e96768",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "\n",
    "***\n",
    "***\n",
    "\n",
    "\n",
    "# Appendix\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "# TFLOPS Calculation\n",
    "\n",
    "### ViT Encoder TFLOPS\n",
    "\n",
    "Formula for the number of floating-point operations (FLOPS) in the convolutional part (forward pass) of the Vision Transformer (ViT):\n",
    "\n",
    "$\\text{FLOPS}_{\\text{conv_forward}} = 2Nhcp^{2}$\n",
    "\n",
    " \n",
    "<br>\n",
    "Where:\n",
    "\n",
    "$h$ : Hidden size\n",
    "\n",
    "$p$ : Patch size\n",
    "\n",
    "$c$ : Image input channels\n",
    "\n",
    "$N$ : Number of image tokens after patchification\n",
    "\n",
    "$l$ : Number of transformer layers\n",
    "\n",
    "\n",
    "Formula for the number of image tokens after patchification:\n",
    "\n",
    "$N = \\left\\lfloor\\frac{W + p - 1}{p}\\right\\rfloor\\times\\left\\lfloor\\frac{H + p - 1}{p}\\right\\rfloor$\n",
    "\n",
    "\n",
    "Formula for the number of floating-point operations (FLOPS) in the Transformer part (forward pass):\n",
    "\n",
    "$\\text{FLOPS}_{\\text{transformer_forward}} = (24Nh^{2}+4hN^{2})l$\n",
    "\n",
    "\n",
    "Formula for the total trillion floating-point operations per second (TFLOPS) of the ViT (forward pass + backward pass) (Note: The classifier head is ignored):\n",
    "\n",
    "$\\text{FLOPS}_{\\text{ViT_(forward+backward)}} = 3\\times((24Nh^{2}+4hN^{2})L + 2Nhcp^{2})$\n",
    "\n",
    "\n",
    "<br><br>\n",
    "\n",
    "### Adaptor/Projector TFLOPS\n",
    "\n",
    "This part mainly aims to align the features from the Vision Transformer (ViT) with the text features. It generally consists of one or two linear layers. Here, we take a single linear layer as an example to calculate the TFLOPs.\n",
    "\n",
    "\n",
    "$\\text{FLOPS}_{\\text{projector_forward}} = 2bs_{vit}h_{vit}h_{llm}$\n",
    "\n",
    "<br><br>\n",
    "\n",
    "### LLM Decoder TFLOPS\n",
    "\n",
    "In most case: $h_2=4h$\n",
    "\n",
    "$\\text{FLOPS}_{\\text{LLM_(forward+backward)}} = 3\\times(24Nh^{2}+4hN^{2})L$\n",
    "\n",
    "$\\text{FLOPS}_{\\text{one_layer_forward}} = 24Nh^{2}+4hN^{2}$\n",
    "\n",
    "If $h_2\\ne4h$:\n",
    "\n",
    "$\\text{FLOPS}_{\\text{one_layer_forward}} = 8Nh^{2}+4hN^{2}+4Nhh_2$\n",
    "\n",
    "$\\text{FLOPS}_{\\text{one_layer_forward_backward}} = 3\\times(8Nh^{2}+4hN^{2}+4Nhh_2)$\n",
    "\n",
    "$\\text{FLOPS}_{\\text{llm_forward_backward}} = 3\\times(8Nh^{2}+4hN^{2}+4Nhh_2)L$\n",
    "\n",
    "Where:\n",
    "\n",
    "$h$ : Hidden size\n",
    "\n",
    "$h_2$ : FFN immediate size\n",
    "\n",
    "$N$ : Sequence length includes images tokens and text tokens\n",
    "\n",
    "$l$ : Number of transformer layers\n",
    "\n",
    "<br>\n",
    "\n",
    "<span style=\"color:red\">**Note:The hyperparameters $l$, $N$, $h$ of the LLM are different from those of the ViT. To distinguish them, we will use $l_{\\text{vit}}$, $N_{\\text{vit}}$, $h_{\\text{vit}}$, $l_{\\text{llm}}$, $N_{\\text{llm}}$, $h_{\\text{llm}}$ to replace them respectively.**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6a19a0",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "****\n",
    "\n",
    "# GPU memory Analysis\n",
    "\n",
    "\n",
    "## GPU Memory Analysis (Transformer Layer)\n",
    "\n",
    "\n",
    "### Parameters\n",
    "\n",
    "Ref:[weights memory](https://nvidia-my.sharepoint.com/:p:/p/xueh/EQ8ZCgcmONpCjfMw9YZcBfsB31BAFcL1pX9oMS_DAIKn9A?e=keCH0v)\n",
    "\n",
    "$Prameters_{\\text{weights_per_transformer_layer}}=2h+12\\frac{h^2}{tp}+4h+\\frac{7h}{tp}=6h+12\\frac{h^2}{tp}+\\frac{7h}{tp}$\n",
    "\n",
    "$Parameters_{\\text{vocab}}=Vh$\n",
    "\n",
    "where:\n",
    "\n",
    "$tp$: tensor parallel size\n",
    "\n",
    "$h$: hidden size\n",
    "\n",
    "here, \n",
    "\n",
    "$h_{\\text{intermediate_size}}=4h$\n",
    "\n",
    "<br>\n",
    "\n",
    "### Total static memory per transformer layer\n",
    "\n",
    "Ref: [total memory](https://nvidia-my.sharepoint.com/:p:/p/xueh/EQ8ZCgcmONpCjfMw9YZcBfsB31BAFcL1pX9oMS_DAIKn9A?e=3DIAlm)\n",
    "\n",
    "Ref: [ZeRO](https://arxiv.org/pdf/1910.02054)\n",
    "\n",
    "If training with BF16, total static memory occupation, grad in 16bit precision:\n",
    "\n",
    "Static memory occupation:\n",
    "\n",
    "$M_{\\text{weights}}=2\\phi$\n",
    "\n",
    "$M_{\\text{grads}}=2\\phi$\n",
    "\n",
    "$M_{\\text{os}}=12\\phi$\n",
    "\n",
    "$M_{\\text{total}}=16\\phi$\n",
    "\n",
    "where:\n",
    "\n",
    "$\\phi$: parameters number\n",
    "\n",
    "So, the total static memory per transformer layer is: <span style=\"color:red\">$M_{\\text{static_memory_per_transformer_layer}}=16\\phi=16*(6h+12\\frac{h^2}{tp}+\\frac{7h}{tp})$</span>\n",
    "\n",
    "### Activation memoy per transformer layer\n",
    "\n",
    "<span style=\"color:red\">$M_{\\text{activation_memory_per_transformer_layer}}=\\frac{34bsh}{tp}$</span>\n",
    "\n",
    "To simplify the calculation, we ingore the activations of vocabulary embedding at the first stage and the LLM Head at the last stage.\n",
    "\n",
    "\n",
    "### Total memory per transformer layer\n",
    "\n",
    "$M_{\\text{per_transformer_layer}}=M_{\\text{static_memory_per_transformer_layer}} + M_{\\text{activation_memory_per_transformer_layer}}$\n",
    "\n",
    "<span style=\"color:red\">$M_{\\text{per_transformer_layer}}=16*(6h+12\\frac{h^2}{tp}+\\frac{7h}{tp})+\\frac{34bsh}{tp}$</span>\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "## GPU Memory Analysis (ViT Encoder)\n",
    "\n",
    "### Parameters (ViT Encoder)\n",
    "\n",
    "$Parameters_{\\text{conv}}=p^2C_{in}C_{out}=p^2ch$\n",
    "\n",
    "$Prameters_{\\text{weights_all_transformer_layers}}=(6h+12\\frac{h^2}{tp}+\\frac{7h}{tp})l$\n",
    "\n",
    "$\\phi=p^2ch+(6h+12\\frac{h^2}{tp}+\\frac{7h}{tp})l$\n",
    "\n",
    "where:\n",
    "\n",
    "$h$ : Hidden size\n",
    "\n",
    "$p$ : Patch size\n",
    "\n",
    "$c$ : Image input channels\n",
    "\n",
    "$N$ : Number of image tokens after patchification\n",
    "\n",
    "$l$ : Number of transformer layers\n",
    "\n",
    "$\\phi$ : Parameters of ViT Encoder\n",
    "\n",
    "### Total static memory (ViT Encoder)\n",
    "\n",
    "$M_{\\text{static_memory_vit_encoder}}=16\\phi=16*(p^2ch+(6h+12\\frac{h^2}{tp}+\\frac{7h}{tp})l)$\n",
    "\n",
    "### Activation memory (ViT Encoder)\n",
    "\n",
    "$M_{\\text{activtion_memory_vit_encoder}}=2whc+\\frac{34bshl}{tp}$\n",
    "\n",
    "### Total memory (ViT Encoder)\n",
    "\n",
    "$M_{\\text{total_vit_encoder}}=M_{\\text{static_memory_vit_encoder}}+M_{\\text{activtion_memory_vit_encoder}}$\n",
    "\n",
    "<span style=\"color:red\">$M_{\\text{total_vit_encoder}}=16*(p^2ch+(6h+12\\frac{h^2}{tp}+\\frac{7h}{tp})l)+2whc+\\frac{34bshl}{tp}$ </span>\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "## GPU Memory Analysis (MLP Adaptor/projector)\n",
    "\n",
    "This part mainly aims to align the features from the Vision Transformer (ViT) with the text features. It generally consists of one or two linear layers. Here, we take a single linear layer as an example to analyze the GPU memory usage.\n",
    "\n",
    "### Parameters (Linear projector)\n",
    "\n",
    "$Parameters_{\\text{projector}}=h_{\\text{vit}}h_{\\text{llm}}$\n",
    "\n",
    "### Total static memory (Linear projector)\n",
    "\n",
    "$M_{\\text{static_memory_projector}}=16\\phi=16h_{\\text{vit}}h_{\\text{llm}}$\n",
    "\n",
    "### Activation memory (Linear projector)\n",
    "\n",
    "$M_{\\text{activtion_memory_projector}}=2b{s_{\\text{vit}}}{h_{\\text{vit}}}$\n",
    "\n",
    "### Total memory (Linear projector)\n",
    "\n",
    "$M_{\\text{total_projector}}=M_{\\text{static_memory_projector}}+M_{\\text{activtion_memory_projector}}$\n",
    "\n",
    "<span style=\"color:red\">$M_{\\text{total_projector}}=16h_{\\text{vit}}h_{\\text{llm}}+2b{s_{\\text{vit}}}{h_{\\text{vit}}}$ </span>\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "## GPU memory on different pp ranks\n",
    "\n",
    "Note: For the ViT Encoder and the LLM Decoder, different settings are used for \n",
    "$h$ and $s$. To make a distinction, we will replace the original $h$ and $s$ with $h_{vit}$ and $s_{vit}$ respectively.\n",
    "\n",
    "The first pp rank:\n",
    "\n",
    "$M_{\\text{first_pp_rank}}=M_{\\text{total_vit_encoder}}+M_{\\text{total_projector}}+M_{\\text{total_llm_on_first_rank}}$\n",
    "\n",
    "<span style=\"color:red\">$M_{\\text{first_pp_rank}}=16(p^2ch_{\\text{vit}}+(6h_{\\text{vit}}+12\\frac{h_{\\text{vit}}^2}{tp}+\\frac{7h_{\\text{vit}}}{tp})l_{\\text{vit}}+2whc+\\frac{34bs_{\\text{vit}}h_{\\text{vit}}l_{\\text{vit}}}{tp} \\\\ \\hspace{4cm} +  16h_{\\text{vit}}h_{\\text{llm}}+2b{s_{\\text{vit}}}{h_{\\text{vit}}} \\\\ \\hspace{4cm} + (16(6h_{\\text{llm}}+12\\frac{h_{\\text{llm}}^2}{tp}+\\frac{7h_{\\text{llm}}}{tp})+\\frac{34bs_{\\text{llm}}h_{\\text{llm}}}{tp})l_{\\text{decoder_first_pipeline_num_layers}}$<span style=\"color:red\">\n",
    "    \n",
    " \n",
    "<span style=\"color:red\">$M_{\\text{other_pp_rank}}=(16(6h_{\\text{llm}}+12\\frac{h_{\\text{llm}}^2}{tp}+\\frac{7h_{\\text{llm}}}{tp})+\\frac{34bs_{\\text{llm}}h_{\\text{llm}}}{tp})l_{\\text{num_layers_per_other_pp_rank}}$</span>\n",
    "    \n",
    "<span style=\"color:red\">$M_{\\text{last_pp_rank}}=(16(6h_{\\text{llm}}+12\\frac{h_{\\text{llm}}^2}{tp}+\\frac{7h_{\\text{llm}}}{tp})+\\frac{34bs_{\\text{llm}}h_{\\text{llm}}}{tp})l_{\\text{num_layers_per_other_pp_rank}} + 16{\\frac{hV}{tp}}+8{\\frac{bsh}{tp}}$</span>\n",
    "    \n",
    "    \n",
    "$M_{\\text{first_pp_rank}}=16(p^2ch_{\\text{vit}}+(6h_{\\text{vit}}+12\\frac{h_{\\text{vit}}^2}{tp}+\\frac{7h_{\\text{vit}}}{tp})l_{\\text{vit}}+2whc+\\frac{34bs_{\\text{vit}}h_{\\text{vit}}l_{\\text{vit}}}{tp} \\\\ \\hspace{4cm} +  16h_{\\text{vit}}h_{\\text{llm}}+2b{s_{\\text{vit}}}{h_{\\text{vit}}} \\\\ \\hspace{4cm} + (16(6h_{\\text{llm}}+12\\frac{h_{\\text{llm}}^2}{tp}+\\frac{7h_{\\text{llm}}}{tp})+\\frac{34bs_{\\text{llm}}h_{\\text{llm}}}{tp})l_{\\text{decoder_first_pipeline_num_layers}}$\n",
    "    \n",
    " \n",
    "$M_{\\text{other_pp_rank}}=(16(6h_{\\text{llm}}+12\\frac{h_{\\text{llm}}^2}{tp}+\\frac{7h_{\\text{llm}}}{tp})+\\frac{34bs_{\\text{llm}}h_{\\text{llm}}}{tp})l_{\\text{num_layers_per_other_pp_rank}}$\n",
    "    \n",
    "$M_{\\text{last_pp_rank}}=(16(6h_{\\text{llm}}+12\\frac{h_{\\text{llm}}^2}{tp}+\\frac{7h_{\\text{llm}}}{tp})+\\frac{34bs_{\\text{llm}}h_{\\text{llm}}}{tp})l_{\\text{num_layers_per_other_pp_rank}} + 16{\\frac{hV}{tp}}+8{\\frac{bsh}{tp}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0687c10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
